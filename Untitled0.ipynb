{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM6dRbH7QSjWfhj5uJJkfg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "df = pd.read_csv('dq_ps_challenge_v2 1.csv')\n",
        "df.replace(['DISPLAY','CLICK','CHECKOUT'],[1,2,3],inplace=True)"
      ],
      "metadata": {
        "id": "Wbcu9Sac_gX1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# =============== BASELINE MODEL =============== #\n",
        "user_interaction_cols = ['idcol','interaction','item']\n",
        "user_interaction_matrix = df[user_interaction_cols]\n",
        "X = user_interaction_matrix.drop('interaction',axis=1)\n",
        "y = user_interaction_matrix['interaction']\n",
        "# we define the number of interactions per user per item as the 'rating' for that product; DISPLAY=1, CLICK=3, CHECKOUT=5\n",
        "\n",
        "# split data into train and valid data\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.05,random_state=69)\n",
        "\n",
        "train_df = X_train.copy()\n",
        "train_df['interaction'] = y_train\n",
        "\n",
        "# Create baseline model that consists of the average of each item's 'rating'\n",
        "baseline_train_y = pd.DataFrame(train_df.groupby('item')['interaction'].mean())\n",
        "baseline_vs_actual_y = pd.merge(train_df,baseline_train_y, on='item').rename(columns={'interaction_y':'predicted_y','interaction_x':'actual'})\n",
        "print(baseline_vs_actual_y)\n",
        "print(\"\\nRMSE FOR BASELINE MODEL:\\n\",math.sqrt(\n",
        "    mean_squared_error(baseline_vs_actual_y['predicted_y'],baseline_vs_actual_y['actual'])\n",
        "))"
      ],
      "metadata": {
        "id": "9X7GyjEr_8TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation Function\n",
        "\n",
        "def evaluate_recommendations(testset, k=10):\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_id = int(uid)\n",
        "\n",
        "        # Get top k hybrid recommendations for the user\n",
        "        hybrid_recommendations = get_hybrid_recommendations(user_id, k)\n",
        "        recommended_products = hybrid_recommendations['product_id'].values\n",
        "\n",
        "        # Get the actual products the user interacted with in the test set\n",
        "        true_products = data[(data['user_id'] == user_id) & (data['rating'] >= 4)]['product_id'].values\n",
        "\n",
        "        # Calculate precision and recall\n",
        "        true_positive = len(set(recommended_products) & set(true_products))\n",
        "        precision = true_positive / len(recommended_products) if recommended_products.size > 0 else 0\n",
        "        recall = true_positive / len(true_products) if true_products.size > 0 else 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "\n",
        "    avg_precision = sum(precisions) / len(precisions)\n",
        "    avg_recall = sum(recalls) / len(recalls)\n",
        "\n",
        "    return avg_precision, avg_recall\n",
        "\n",
        "# Example: Get evaluation metrics for the hybrid recommendations\n",
        "avg_precision, avg_recall = evaluate_recommendations(testset, k=10)\n",
        "print(f\"\\nAverage Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "tvnlgbc_As61",
        "outputId": "c231b635-b534-4d6b-ae3e-3c91b04dff0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'testset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea78c96f86cc>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Example: Get evaluation metrics for the hybrid recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mavg_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nAverage Precision: {avg_precision:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Recall: {avg_recall:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testset' is not defined"
          ]
        }
      ]
    }
  ]
}